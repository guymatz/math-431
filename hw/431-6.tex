\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[version=4]{mhchem}
\usepackage{stmaryrd}
\usepackage{tikz}
%\usepackage{enumitem}
\usepackage[inline]{enumitem}

%\input{.tex/preamble}
%\input{.tex/macros}
%\input{.tex/letterfonts}

\title{UW - Math 431 \\
Probability Theory \\
Homework 6}

\author{Guy Matz}
\date{\today}


\begin{document}
\maketitle

\begin{itemize}
  \item[6.28] Let $X$ and $Y$ be independent $\operatorname{Geom}(p)$ random variables. Let $V=\min (X, Y)$ and

\[
W= \begin{cases}0, & \text { if } X<Y \\ 1, & \text { if } X=Y \\ 2, & \text { if } X>Y\end{cases}
\]

Find the joint probability mass function of $V$ and $W$ and show that $V$ and $W$ are independent.

Hint. Use the joint probability mass function of $X$ and $Y$ to compute the joint probability mass function of $V$ and $W$.


\newpage
  \item[6.36] Suppose that $X, Y$ are jointly continuous with joint probability density function

\[ f(x, y)=c e^{-\frac{x^{2}}{2}-\frac{(x-y)^{2}}{2}}, \quad x, y \in(-\infty, \infty) \]
for some constant $c$.

    \begin{enumerate}
       \item Find the value of the constant $c$.

          Integrating $f(x,y)$ we get
          \[ \int_{-\infty}^{\infty} \int_{-\infty}}^{\infty} {f(x,y)} d{x} d{y} = 2 \pi \]
          So $c = \frac{1}{2\pi} $

       \item Find the marginal density functions of $X$ and $Y$.

          \[ f_X(x) = \frac
               {e^
                  {
                     -\frac{x^2}{2}
                  }
               }
               {\sqrt{2\pi}}  \]
          \[ f_Y(y) = \frac {e^ { -\frac{y^2}{4} } } {2\sqrt{\pi}}
          \]

       \item Determine whether $X$ and $Y$ are independent.

          $X$ and $Y$ are \underline{NOT} independent
          since
          \[ f_X(x) \cdot f_Y(y) = \sqrt{2 \pi} e^ { \frac{-2x^2 - y^2}{4} } \neq  \frac{1}{2\pi} e^{-\frac{x^{2}}{2}-\frac{(x-y)^{2}}{2}} \]
    \end{enumerate}

\newpage
   \item[7.5] Let $X, Y$ and $Z$ be independent normal random variables with distributions $X \sim \mathcal{N}(1,2), Y \sim \mathcal{N}(2,1)$, and $Z \sim \mathcal{N}(0,7)$. Let $W=X-4 Y+Z$.
     \begin{enumerate}
       \item Identify the distribution of $W$.
          \[ E[W] = 1 - 4 \cdot (2) + 0 = -7 \]
          \[ \operatorname{Var}(W) = 2 + 16 \cdot (1) + 7 = 25 \]
          So
          \[ W \sim \mathcal{N}(-7,25)  \]

       \item Find the probability $P(W>-2)$.
          \begin{align*}
             P\left(W > -2\right) &= P\left(\frac{W- -7}{\sqrt{25}} > \frac{-2 - -7}{\sqrt{25}} \right)\\
             &= P\left(\frac{W- -7}{\sqrt{25}} > \frac{5}{5} )\\
             &= P\left(\frac{W- -7}{\sqrt{25}} > 1\right) \\
             &= 1 - P\left(\frac{W- -7}{\sqrt{25}} < 1\right) \\
             &= 1 - \Phi\left( 1\right) \\
             &= 1 - 0.8413 \\
             &= 0.1586 \\
          \end{align*}
     \end{enumerate}

\newpage
   \item[7.7] Let $X_{1}, X_{2}, X_{3}$, and $X_{4}$ be independent standard normal random variables. What is the probability that $X_{3}$ is the second largest?


      Since $X_n$ are i.i.d and continuous they are
      exchangable.  So the probability that $X_3$ will
      be second largest is
      \[ \frac{\#A}{\#\Omega} = \frac{3 \cdot 1 \cdot 2 \cdot 1}{4 \cdot 3 \cdot 2 \cdot 1} = \frac{1}{4}    \]
\newpage
   \item[7.16] Let $X$ be a Poisson random variable
      with parameter $\lambda$, and $Y$ an independent
      Bernoulli random variable with parameter $p$.
      Find the probability mass function of $X+Y$.

      \begin{align*}
         P(X+Y=n) = P_X * P_Y(n) &= \sum_{k} p_X(k)p_Y(n-k) \\
             &= \sum^{n}_{k=0} p_X(k)p_Y(n-k) \\
             &= \sum^{1}_{k=0} p_X(k)p_Y(n-k) \\
             &= P(X=0) \cdot P(Y=1)
                  + P(X=1) \cdot P(Y=0) \\
             &= p e^{-\lambda} + (1-p) \lambda e^{-\lambda}
      \end{align*}
\newpage
   \item[8.9] Suppose $X$ and $Y$ are independent
      random variables with $E[X]= 3, E[Y]=5,
      \operatorname{Var}(X)=2$ and
      $\operatorname{Var}(Y)=3$. Compute the following
      quantities.

    \begin{enumerate}
       \item $E[3X - 2Y +7]$
          \[ 3 \cdot 3 - 2 \cdot 5 + 7 = 6 \]

       \item $\operatorname{Var}(3 X-2 Y+7)$
          \[ 3^2 \cdot 2 + 2^2 \cdot 3 = 18 + 12 = 30 \]

       \item $\operatorname{Var}(X Y)$
          \begin{align*}
             \operatorname{Var}(X Y) &= E\left[X^2 Y^2\right]-(E[X Y])^2 \\
                                     &= E\left[X^2] \cdot E\left[ Y^2\right]-(E\left[X\right] \cdot E \left[Y \right] )^2 \\
              &= \operatorname{Var}(X) \operatorname{Var}(Y) +\operatorname{Var}(X)(E[Y])^2+\operatorname{Var}(Y)(E[X])^2
          \end{align*}
          So
          \[ \operatorname{Var}(XY) = 2 \cdot 3 + 2 \cdot 5^2 + 3 \cdot 3^2 = 6 + 50 + 27 = 83 \]
    \end{enumerate}

\newpage
   \item[8.12]

     \begin{enumerate}
       \item Let $Z$ be Gamma(2, $\lambda)$ distributed. That is, $Z$ has the density function

          \[ f_{Z}(z)= \begin{cases}\lambda^{2} z e^{-\lambda z}, & z \geq 0 \\ 0, & \text { otherwise }\end{cases} \]

Use the definition of the moment generating function to calculate $M_{Z}(t)=E\left(e^{t Z}\right)$

   \begin{align*}
   M_Z(t) &= E[e^{t z }\lambda^2 z e^{-\lambda z}] \\
          &= \int_{0}^{\infty} e^{t z }\lambda^2 z e^{-\lambda z} d{z}\\
          &= \int_{0}^{\infty} \lambda^2 z e^{z(t -\lambda)} d{z}\\
          &= \lambda^2 \left( \frac{ze^{z(t-\lambda)}}{t-\lambda}\bigg\rvert_0^{\infty} - \int_{0}^{\infty} \frac{e^{z(t-\lambda)}}{t- \lambda} d{z} \right)\\
          &= \lambda^2 \left( \frac{ze^{z(t-\lambda)}}{t-\lambda}- \frac{e^{z(t-\lambda)}}{(t- \lambda)^2} \right) \bigg\rvert_0^{\infty} \\
          &= \lambda^2 \left( 0 - 0 - (0 -  \frac{1}{(t- \lambda)^2} \right) \\
          &= \lambda^2 \frac{1}{(t- \lambda)^2} \right) \\
          &= \frac{\lambda^2}{( \lambda-t)^2} \right) \\
   \end{align*}
   When $t < \lambda$,  Otherwise $M_Z(t) = \infty$

       \item Let $X$ and $Y$ be two independent $\operatorname{Exp}(\lambda)$ random variables. Recall the moment generating function of $X$ and $Y$ from Example 5.6 in Section 5.1. Using the approach from Section 8.3 show that $Z$ and $X+Y$ have the same distribution.

          From $\S 5.1$
          \[
         M(t)= \begin{cases}\infty, & \text { if } t \geq \lambda \\ \frac{\lambda}{\lambda-t}, & \text { if } t<\lambda\end{cases}
         \]
          From $\S 8.3$
          \[ M_{X+Y}(t)=M_X(t) M_Y(t) \]

          So
          \begin{align*}
             M_{X+Y}(t) &= \frac{\lambda}{\lambda-t} \cdot \frac{\lambda}{\lambda-t} \\
             &= \left( \frac{\lambda}{\lambda-t} \right)^2
          \end{align*}


     \end{enumerate}

\newpage
   \item[8.42] The random variables $X_{1}, \ldots, X_{n}$ are i.i.d. We also know that $E\left[X_{1}\right]=0, E\left[X_{1}^{2}\right]=a, E\left[X_{1}^{3}\right]=b$ and $E\left[X_{1}^{4}\right]=c$. Let $\bar{X}_{n}=\frac{X_{1}+\cdots+X_{n}}{n}$. Find the fourth moment of $\bar{X}_{n}$.

\newpage
   \item[8.48] Suppose there are 100 cards numbered 1 through 100. Draw a card at random. Let $X$ be the number of digits on the card (between 1 and 3) and let $Y$ be the number of zeros. Find $\operatorname{Cov}(X, Y)$.

\newpage
   \item[8.54] Suppose that for the random variables $X, Y$ we have $E[X]=$ $2, E[Y]=1, E\left[X^{2}\right]=5, E\left[Y^{2}\right]=10$ and $E[X Y]=1$

     \begin{enumerate}
       \item Compute $\operatorname{Corr}(X, Y)$.

       \item Find a number $c$ so that $X$ and $X+c Y$ are uncorrelated.
     \end{enumerate}
\end{itemize}
\end{document}
