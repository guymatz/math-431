\documentclass{report}

\input{.tex/preamble}
\input{.tex/macros}
\input{.tex/letterfonts}

\title{
  \Huge{Math 431 - Introduction to Probability Theory}
  \\
  Notes
}
\author{\huge{Guy Matz}}
\date{}

\begin{document}
%\maketitle

% \setcounter{chapter}{1}
\chapter{20230619 - Introduction}

\begin{itemize}
  \item Reiew of set nontation and DeMorgan's rule (Appendix B, C)

    \begin{itemize}

    \item $\Omega$ is a set and contains some elements.  if $\Omega$
      contains $\omega$ we write $\omega \in \Omega$

    \item A is a subset of B: $A \subset B$

    Union, interection, complement . . .\\
    Assume A and B are subsets of $\Omega$
    $$A \cup B = \{ \omega \in \Omega: \omega \in A OR \omega \in B\}$$
    $$A \cap B = \{ \omega \in \Omega: \omega \in A AND \omega \in B\}$$
    $$A^c = \{ \omega \in \Omega: \omega \notin A \}$$
    $$A \setminus B=\{\omega \in \Omega:\omega \in A AND \omega \notin B\}$$
    \[ A \setminus B = A \cap B^c \]

    DeMorgan's Law: $\left( \cap A_i \right)^c = \cup^{\infty}_{i=1} A_i^c$

    \end{itemize}

  \item Definitions
    \dfn{ Probability }{
      Mathematical models of experiments (See video @ min 32)
    }

    \dfn{ Sample space  }{
      $\Omega$: set of possible outcomes
    }

    \dfn{ Sample Points }{
      $\omega$: Elements of $\Omega$ are \underline{Sample points}
    }

    \dfn{ Event Space }{
      $\mathcal{F}$: Subsets of $\Omega$
    }

    \dfn{ Probability Measure }{
    $P$: Probability Measure == Probability Distribution == Probability
    }

    \dfn{ Probability Space }{
      $(\Omega, \mathcal{F}, P)$
    }
  \end{itemize}

\chapter{20230620 - Random Sampling}
\begin{itemize}
  \item Order - objects chosen one ata time
  \item unOrder - objects chosen in a group
  \item With Replacement - 
  \item WithOUT Replacement - 
  \item ORDERed with replacement
    \[ P(\omega) = \left( \frac{1}{\#\Omega} \right) = \left( \frac{1}{n} \right)^k \]
  \item ORDERed withOUT replacement (falling factorial)
  \[ P(\omega) = \frac{1}{(n)_k} = \frac{k!}{n!} \]
  \[ \#\Omega = (n)_k \]
  \[ \#A = \text{\# of slots for each  option} \cdot \text{\# of each option} \]
  \item UNordered withOUT replacement - "chosen all at once"
    \[ \#\Omega = \binom{n}{k} = \frac{n!}{(n-k)!k!}  \]
    \[ \#A = \prod_{j=0}^{m} \binom{n_j}{k_j} \]

\end{itemize}

\chapter{20230622}%
\section*{Random Varaibles}%

  \dfn{ Random Variable }{
    A function from $\Omega$ into the real numbers
  }
  \dfn{ Probability Distribution }{
    The Collection of probabilites $P\{X \in B \}$ for sets $B \subset R$ 

    An assignment of probabilities to subsets of $R$ that satisfies
    the axioms of probability
  }
  \dfn{ Discrete RV }{
    an RV is discrete if there exists a finite or countably infinite
    set $\{k_1, k_2,,,\}$ such that the sum ranges 

    bodyRead up on this
  }
  \dfn{ Probability Mass Function - p.m.f }{
    \[ p(k) = P(X=k) \]
    Describes a discrete RV.  The function $p_X$ gives the probability
    of each possible value of X.
  }

  \dfn{ Conditional Prob }{
    Let BB be an event in the sample space such that $P(B) > 0$.
    THen for all events A, the conditional prob of "A given B" is
    deffined by 
    \[ P(A|B) = \frac{P(A \cap B)}{P(B)} = \frac{P(A B)}{P(B)}  \]
  }

  \dfn{ Partition }{
    A finite collection off events $\{B_1..B_n\}$ is a \underline{partition}
    off $\Omega $ if the sets $B_i$ are pairwise disjoint and together
    they make up $\Omega$.  That is, $B_iB_j = \emptyset$ when $i \neq
    j$ and $\cup^n_{i=1}B_i = \Omega$
  }

  



\chapter{20230627}%
  \begin{itemize}
    \item Independence
      \begin{enumerate}
        \item get 3 different version
      \end{enumerate}
  \end{itemize}
\chapter{20230710}%
  \section*{LLN and CLT}%
    \subsection*{Markov's Inequality}%
    \dfn{ Markov's Inequality }{
      Let $X$ be a non-negative RV.  Then ffor any $c > 0$,
      \[ P(X \geq c) \leq \frac{E[X]}{c}  \]
    }
    \myproof {
    \[E[X] \geq E[c\cdot 1_{X \geq c}] =c \cdot P(A) =c \cdot P(X \geq c)\]
    }
    \nt{
      The advantage o Markov's Inequality is that it only requires
      knowledge of the mean of X to provide a bound.  However, it only
      gives an upper bound and is not "sharp", i.e. it may be off the
      mark.
    }

    \subsection*{Chebyshev's Inequality}
    \dfn{ Chebyshev's Inequality }{
      Let $X$ be a RV with a finite mean, $\mu$ and finite variance,
      $\sigma^2$.  Then ffor any $c \geq 0$ we have
      \[ P(|X - \mu| \geq c) \leq \frac{var(X)}{c^2}  \]
    }
    \myproof {
      $P(|X) \geq c) = P(|X - \mu| \geq c - \mu) \leq \frac{Var(X)}{c^2}$
    }

\chapter{20230718}%
  \section*{Distribution of a Function of a RV}%
  
    \subsection*{Discrete}%
    
    \begin{align*}
      E[Y] &= \sum^{}_{l} l \cdot P(Y=l)\\ 
           &= \sum^{}_{l} l \sum^{}_{k: g(k)=l} P(X=k)\\ 
           &= \sum^{}_{l} \sum^{}_{k: g(k)=l} g(k) P(X=k)\\ 
           &= \sum^{}_{k} g(k) P(X=k)\\ 
    \end{align*}
    So, all possible values of Y = range of g

    \subsection*{Continuous}%
    $g$ is diff'able and $g'(x)$ exists, and $\neq 0$

    \thm{To compute the pdf of Y}{
      Derive the cdf $F_Y$ of $Y=g(x)$ and differentiate to find $f(y) = F'(y)$
    }

    See example at min 19 of 20230718

    Function must be increasing (or decreasing?)!!  Then
    \begin{align*}
      f_Y(y) &= \frac{d F_Y(y)}{dy} \\
             &= \frac{d F_X(g^{-1}(y))}{dy} \\
             &= f_X(g^{-1}(y)) \cdot \frac{d g^{-1}(y)}{dy}  \\
             &= f_X(g^{-1}(y)) \cdot \frac{1}{g'(g^{-1}(y))}
    \end{align*}




\chapter{20230719}%
  \section*{Chapter 6 - Joint Distribution of RVs}%
  \dfn{\underline{Marginal Probability Function}}{
    The sum of all possible values for a RV, E.g. for
    two RVs Y and Y, the P that $X=k$
    \[ p_X(k) = P(X=k) = \sum^{}_{l \in Y} p(k,l) \]
    And the P that $Y=l$
    \[ P_Y(l) = P(Y=l) = \sum^{}_{k \in X} p(k,l) \]
  }
  \dfn{ Generalized $_nC_k$ }{
    \[ \binom{n}{k_1,k_2\dots , k_n} = \frac{n!}{k_1!k_2! \dots k_r!} \]
  }
  \dfn{ Multinomial Distribution }{
    \[ P(X_1 = k_1, \dots X_n=k_n) = \binom{n}{k_1,k_2\dots , k_n} p_1^{k_1} p_2^{k_2} \dots p_r^{k_r} \]
      Abbreviated by:
      \[ (X_1, X-2, \dots X_r) \sim \text{Mult}(n, r_1, p_1, p_2, \dots p_r) \]
  }
  \section{Joint Continuous RV}%
    \dfn{ Joint Continuous }{
      RVs $X_1, \dots X_n$ are Joint Continuous if there exists a 
      Joint Distribution Function $f$ on $R^n$ such that 
      for subsets $B \subset R^n$,
      \[ P((X_1, \dots X_n) \in B) = \int \int_{B} f(x_1, \dots x_n) dx_1 dx_2 \dots dx_n \]
    }
    
  
  
\chapter{Exam Notes}%
  \section*{Exam I}%
    \subsection*{Discrete Distributions}%
      \begin{itemize}
        \item Binomial
          \begin{itemize}
            \item use-case: Many trials of Two possible outcome
            \item pmf: $P(X=k) = \binom{n}{k} p^k (1-p)^{n-k}$
          \end{itemize}
        \item Geometric
          \begin{itemize}
            \item use-case: Num of failures before a success
            \item pmf: $P(X=k) = p \cdot (1-p)^{k-1}$
          \end{itemize}
        \item Hypergeometric
          \begin{itemize}
            \item use-case: Sample drawn without replacement
            \item pmf: $P(X=k) = \frac{\binom{N_A}{k}\binom{N-N_A}{n-k}}{\binom{N}{n}} $
          \end{itemize}

      \end{itemize}
    \subsection*{Continuous Distributions}%
      \begin{itemize}
        \item Conditions For Contimuous Distribution
          \begin{itemize}
            \item Continuous \& Differentiable
            \item pdf integrates to 1
          \end{itemize}
        \item Normal
          \begin{itemize}
            \item Estimation metric:  $np(1-p) > 10$
          \end{itemize}
        \item Poisson
            \begin{itemize}
              \item pdf: $\frac{\lambda^k}{k!} e^{-\lambda}$
              \item Estimation metric: $np(1-p) < 10$
            \end{itemize}
      \end{itemize}
    \subsection*{Independence}%
      \begin{itemize}
        \item Multiple events (and their complemets) are independent if
          \[ P(A B)=P(A) P(B), P(A C)=P(A) P(C), P(B C)=P(B) P(C), P(A B C)=P(A) P(B) P(C) \]
        \item Inclusion / Exclusion
          \[ P(A \cup B \cup C) = P(A) +  P(B) + P(C) - P(A \cap B) - P(B \cap C) - P(A \cap C) + P(A \cap B \cap C)\]
        \item Conditional Independence
      \end{itemize}
    
\end{document}
